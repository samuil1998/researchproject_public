{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "from random import random \n",
    "from random import sample \n",
    "from scipy.stats import norm\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn import svm, datasets\n",
    "import matplotlib \n",
    "\n",
    "matplotlib.rc('font', **{'size'   : 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"../preprocessing/standardized_data.csv\"\n",
    "data = pd.read_csv(DATA,header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Again, reusing the misclassification definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_misclassification(gt, prediction): \n",
    "    \"\"\"Given a vector of ground truth values `gt` and a vector of prediction values `gt`, \n",
    "    return a vector containing 1 if there has been gender misclassification and 0 otherwise. \"\"\"\n",
    "    n = len(gt)\n",
    "    assert(len(prediction) == n)\n",
    "    result = np.zeros(n)\n",
    "    for i in range(n): \n",
    "        # If the ground truth or the result from the API is \"unsure\", no misclassification\n",
    "        if gt[i] == 0 or prediction[i] == 0: \n",
    "            result[i] = 0\n",
    "        # If the ground truth does not match the prediction, set the misclassification bit to one. \n",
    "        elif gt[i] != prediction[i]: \n",
    "            result[i] = 1\n",
    "        else: \n",
    "            result[i] = 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def race_misclassification(gt, prediction): \n",
    "    \"\"\"Given a vector of ground truth values `gt` and a vector of prediction values `gt`, \n",
    "    return a vector containing 1 if there has been race misclassification and 0 otherwise. \"\"\"\n",
    "    n = len(gt)\n",
    "    assert(len(prediction) == n)\n",
    "    result = np.zeros(n)\n",
    "    for i in range(n): \n",
    "        # If the subjects is within a cornercase group or the prediction has outputed \"not sure\", then \n",
    "        # no misclassification\n",
    "        if gt[i] in [0,5] or prediction[i] == 0 : \n",
    "            result[i] = 0\n",
    "        # If the ground truth does not match the prediction, set the misclassification bit to one. \n",
    "        elif gt[i] != prediction[i]: \n",
    "            result[i] = 1\n",
    "        else: \n",
    "            result[i] = 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_misclassification(gt, prediction): \n",
    "    \"\"\"Given a vector of ground truth values `gt` and a vector of prediction values `gt`, \n",
    "    return a vector containing 1 if there has been age misclassification and 0 otherwise. \"\"\"\n",
    "    n = len(gt)\n",
    "    assert(len(prediction) == n)\n",
    "    result = np.zeros(n)\n",
    "    \n",
    "    for i in range(n): \n",
    "        tolerance = 0 \n",
    "        # If the subject is a child, tolerance interval equals 5\n",
    "        if gt[i] <= 10: \n",
    "            tolerance = 5\n",
    "        # For teenagers and pre-young-adults, tolerance is 10\n",
    "        elif gt[i] <= 25: \n",
    "            tolerance = 10\n",
    "        # For the rest of the population, tolerance is 15\n",
    "        else: \n",
    "            tolerance = 15\n",
    "            \n",
    "        if gt[i] - tolerance <= prediction[i] <= gt[i] + tolerance: \n",
    "            result[i] = 0\n",
    "        else: \n",
    "            result[i] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_misclassification(gt, prediction): \n",
    "    \"\"\"Given a vector of ground truth values `gt` and a vector of prediction values `gt`, \n",
    "    return a vector containing 1 if there has been emotion misclassification and 0 otherwise. \"\"\"\n",
    "    n = len(gt)\n",
    "    assert(len(prediction) == n)\n",
    "    result = np.zeros(n)\n",
    "    \n",
    "    for i in range(n): \n",
    "        # If dealing with one of the undefined emotions, set the misclassification bit to 0\n",
    "        if gt[i] <= 0 or prediction[i] <= 0: \n",
    "            result[i] = 0 \n",
    "        # If emotions don't match, set the misclassification bit to 1\n",
    "        elif gt[i] != prediction[i]: \n",
    "            result[i] = 1\n",
    "        else: \n",
    "            result[i] = 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(estimator, X, y): \n",
    "    \"\"\" Calculate the recall of the estimator predicting on inputs X with output labels y\"\"\"\n",
    "\n",
    "    prediction = estimator.predict(X)\n",
    "    \n",
    "    y = np.array(y)\n",
    "    prediction = np.array(prediction)\n",
    "        \n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for i in range(y.size): \n",
    "        if y[i] == 1 and prediction[i] == 1:\n",
    "            tp += 1\n",
    "            continue \n",
    "        if y[i] == 1 and prediction[i] == 0: \n",
    "            fn += 1\n",
    "            \n",
    "    return (tp / (tp + fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(estimator, X, y):\n",
    "    \"\"\" Calculate the precision of the estimator predicting on inputs X with output labels y\"\"\"\n",
    "    \n",
    "    prediction = estimator.predict(X)\n",
    "    \n",
    "    y = np.array(y)\n",
    "    prediction = np.array(prediction)\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for i in range(y.size):\n",
    "        if y[i] == 1 and prediction[i] == 1:\n",
    "            tp += 1\n",
    "            continue\n",
    "        if y[i] == 0 and prediction[i] == 1:\n",
    "            fp += 1\n",
    "    return (tp / (tp + fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(model, data=None, outputs=None): \n",
    "    \"\"\"Given a classifier, display the cross-validation score on data (by default X_scaled) using \n",
    "    expected_cost, recall, precision and accuracy and scoring functions.\"\"\" \n",
    "    if data is None: \n",
    "        data = X_scaled\n",
    "    if outputs is None: \n",
    "        outputs = y \n",
    "    recall_scores = cross_val_score(model, data, outputs, cv=5, scoring=recall)\n",
    "    precision_scores = cross_val_score(model, data, outputs, cv=5, scoring=precision)\n",
    "    accuracy_scores = cross_val_score(model, data, outputs, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "    rec = sum(recall_scores)/10 \n",
    "    prec = sum(precision_scores)/10\n",
    "    acc = sum(accuracy_scores)/10\n",
    "    \n",
    "    df = pd.DataFrame((rec, prec, acc), index=[\"Recall\", \"Precision\", \"Accuracy\"], columns=[\"Performance\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Misclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_ai = data[data[\"Origin\"] != \"AI\"]\n",
    "gender_misclassification(data_no_ai[\"Gender\"], data_no_ai[\"clarifai_gender\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = gender_misclassification(data_no_ai[\"Gender\"], data_no_ai[\"clarifai_gender\"]) == 1\n",
    "ms = gender_misclassification(data_no_ai[\"Gender\"], data_no_ai[\"microsoft_gender\"]) == 1\n",
    "am = gender_misclassification(data_no_ai[\"Gender\"], data_no_ai[\"amazon_gender\"]) == 1\n",
    "fa = gender_misclassification(data_no_ai[\"Gender\"], data_no_ai[\"face++_gender\"]) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_misclass = (cl | ms | am |fa)\n",
    "sum(gender_misclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_no_ai[[\"Race\", \"Age\", \"Gender\", \"Emotion\"]]\n",
    "y = [int(x) for x in gender_misclass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "X_eng = X.copy()\n",
    "\n",
    "is_black = [int(x) for x in X_eng[\"Race\"] == 2]\n",
    "is_white = [int(x) for x in X_eng[\"Race\"] == 4]\n",
    "is_asian = [int(x) for x in X_eng[\"Race\"] == 1]\n",
    "is_latino = [int(x) for x in X_eng[\"Race\"] == 3]\n",
    "is_rest = [int(x) for x in X_eng[\"Race\"] == 5]\n",
    "\n",
    "is_happy = [int(x) for x in X_eng[\"Emotion\"] == 5]\n",
    "is_sad = [int(x) for x in X_eng[\"Emotion\"] == 6]\n",
    "is_calm = [int(x) for x in X_eng[\"Emotion\"] == 2]\n",
    "is_angry = [int(x) for x in X_eng[\"Emotion\"] == 1]\n",
    "is_fearful = [int(x) for x in X_eng[\"Emotion\"] == 4]\n",
    "is_surprised = [int(x) for x in X_eng[\"Emotion\"] == 7]\n",
    "is_disgusted = [int(x) for x in X_eng[\"Emotion\"] == 3]\n",
    "\n",
    "X_eng[\"is_black\"] = is_black\n",
    "X_eng[\"is_white\"] = is_white\n",
    "X_eng[\"is_asian\"] = is_asian\n",
    "X_eng[\"is_latino\"] = is_latino\n",
    "X_eng[\"is_rest\"] = is_rest\n",
    "X_eng[\"is_happy\"] = is_happy\n",
    "X_eng[\"is_sad\"] = is_sad\n",
    "X_eng[\"is_calm\"] = is_calm\n",
    "X_eng[\"is_angry\"] = is_angry\n",
    "X_eng[\"is_fearful\"] = is_fearful\n",
    "X_eng[\"is_surprised\"] = is_surprised\n",
    "X_eng[\"is_disgusted\"] = is_disgusted\n",
    "\n",
    "X = X_eng.drop(columns=[\"Race\", \"Emotion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='lbfgs') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(cross_val_score(lr, X_train_scaled, y_train, cv=5))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train_scaled, y_train)\n",
    "prediction = lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_actually_misclass = np.array(y_test) == 1\n",
    "is_not_misclass = np.array(y_test) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(prediction[is_actually_misclass] == 1)/sum(is_actually_misclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(prediction[is_not_misclass] == 0)/sum(is_not_misclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_accuracy(estimator, X, y):\n",
    "    \"\"\" Calculate the precision of the estimator predicting on inputs X with output labels y\"\"\"\n",
    "    \n",
    "    prediction = np.array(estimator.predict(X))\n",
    "    y = np.array(y)\n",
    "    \n",
    "    is_actually_misclass = y == 1\n",
    "    is_not_misclass = y == 0\n",
    "    \n",
    "    acc_for_1 = sum(prediction[is_actually_misclass] == 1)/sum(is_actually_misclass)\n",
    "    acc_for_0 = sum(prediction[is_not_misclass] == 0)/sum(is_not_misclass)\n",
    "    \n",
    "    return (acc_for_0 + acc_for_1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='lbfgs') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(cross_val_score(lr, X_train_scaled, y_train, cv=5, scoring=balanced_accuracy))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "best_weight = None \n",
    "best_score = 0 \n",
    "for w in np.linspace(1,30,50): \n",
    "    lr_tune = LogisticRegression(solver='lbfgs', class_weight={0:1, 1:w})\n",
    "    cv_score = sum(cross_val_score(lr_tune, X_train_scaled, y_train, cv=5, scoring=balanced_accuracy))/5\n",
    "    if cv_score > best_score: \n",
    "        best_weight = w\n",
    "        best_score = cv_score\n",
    "    res.append(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(np.linspace(1,30,50), res)\n",
    "plt.xlabel(\"class_weight value\")\n",
    "plt.ylabel(\"Balanced accuracy\")\n",
    "#plt.savefig('training.svg', format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_weight)\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_balanced = LogisticRegression(solver='lbfgs', class_weight={0:1, 1:best_weight}) \n",
    "sum(cross_val_score(lr_balanced, X_train_scaled, y_train, cv=5, scoring=balanced_accuracy))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_balanced.fit(X_train_scaled, y_train)\n",
    "balanced_prediction = lr_balanced.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy for the 1-class\")\n",
    "print(1 - sum(abs(balanced_prediction[np.array(y_test) == 1] - 1))/sum(np.array(y_test) == 1))\n",
    "print()\n",
    "print(\"Accuracy for the 0-class\")\n",
    "print(1 - sum(abs(balanced_prediction[np.array(y_test) == 0] - 0))/sum(np.array(y_test) == 0))\n",
    "print()\n",
    "print(\"Balanced accuracy score: \")\n",
    "print(balanced_accuracy(lr_balanced, X_test_scaled, y_test))\n",
    "# Same as (acc_0 + acc_1)/2\n",
    "print()\n",
    "print(\"Weighted accuracy\")\n",
    "print(1 - sum(abs(balanced_prediction - y_test))/len(y_test))\n",
    "print()\n",
    "print(\"Recall\")\n",
    "print(recall(lr_balanced, X_test_scaled, y_test))\n",
    "print()\n",
    "print(\"Precision\")\n",
    "print(precision(lr_balanced, X_test_scaled, y_test))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We can see the most important factors for gender misclassification in out model are gender itself and age.\")\n",
    "lr_balanced.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_balanced.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(zip(X.columns, lr_balanced.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Race Misclassification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_ai = data[data[\"Origin\"] != \"AI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_misclass = race_misclassification(data_no_ai[\"Race\"], data_no_ai[\"clarifai_race\"]) == 1\n",
    "sum(race_misclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_no_ai[[\"Race\", \"Age\", \"Gender\", \"Emotion\"]]\n",
    "y = [int(x) for x in race_misclass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "X_eng = X.copy()\n",
    "\n",
    "is_black = [int(x) for x in X_eng[\"Race\"] == 2]\n",
    "is_white = [int(x) for x in X_eng[\"Race\"] == 4]\n",
    "is_asian = [int(x) for x in X_eng[\"Race\"] == 1]\n",
    "is_latino = [int(x) for x in X_eng[\"Race\"] == 3]\n",
    "is_rest = [int(x) for x in X_eng[\"Race\"] == 5]\n",
    "\n",
    "is_happy = [int(x) for x in X_eng[\"Emotion\"] == 5]\n",
    "is_sad = [int(x) for x in X_eng[\"Emotion\"] == 6]\n",
    "is_calm = [int(x) for x in X_eng[\"Emotion\"] == 2]\n",
    "is_angry = [int(x) for x in X_eng[\"Emotion\"] == 1]\n",
    "is_fearful = [int(x) for x in X_eng[\"Emotion\"] == 4]\n",
    "is_surprised = [int(x) for x in X_eng[\"Emotion\"] == 7]\n",
    "is_disgusted = [int(x) for x in X_eng[\"Emotion\"] == 3]\n",
    "\n",
    "X_eng[\"is_black\"] = is_black\n",
    "X_eng[\"is_white\"] = is_white\n",
    "X_eng[\"is_asian\"] = is_asian\n",
    "X_eng[\"is_latino\"] = is_latino\n",
    "X_eng[\"is_rest\"] = is_rest\n",
    "X_eng[\"is_happy\"] = is_happy\n",
    "X_eng[\"is_sad\"] = is_sad\n",
    "X_eng[\"is_calm\"] = is_calm\n",
    "X_eng[\"is_angry\"] = is_angry\n",
    "X_eng[\"is_fearful\"] = is_fearful\n",
    "X_eng[\"is_surprised\"] = is_surprised\n",
    "X_eng[\"is_disgusted\"] = is_disgusted\n",
    "\n",
    "X = X_eng.drop(columns=[\"Race\", \"Emotion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='lbfgs') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(cross_val_score(lr, X_train_scaled, y_train, cv=5))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train_scaled, y_train)\n",
    "prediction = lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_actually_misclass = np.array(y_test) == 1\n",
    "is_not_misclass = np.array(y_test) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(prediction[is_actually_misclass] == 1)/sum(is_actually_misclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(prediction[is_not_misclass] == 0)/sum(is_not_misclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_accuracy(estimator, X, y):\n",
    "    \"\"\" Calculate the precision of the estimator predicting on inputs X with output labels y\"\"\"\n",
    "    \n",
    "    prediction = np.array(estimator.predict(X))\n",
    "    y = np.array(y)\n",
    "    \n",
    "    is_actually_misclass = y == 1\n",
    "    is_not_misclass = y == 0\n",
    "    \n",
    "    acc_for_1 = sum(prediction[is_actually_misclass] == 1)/sum(is_actually_misclass)\n",
    "    acc_for_0 = sum(prediction[is_not_misclass] == 0)/sum(is_not_misclass)\n",
    "    \n",
    "    return (acc_for_0 + acc_for_1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='lbfgs') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(cross_val_score(lr, X_train_scaled, y_train, cv=5, scoring=balanced_accuracy))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "best_weight = None \n",
    "best_score = 0 \n",
    "for w in np.linspace(1,30,50): \n",
    "    lr_tune = LogisticRegression(solver='lbfgs', class_weight={0:1, 1:w})\n",
    "    cv_score = sum(cross_val_score(lr_tune, X_train_scaled, y_train, cv=5, scoring=balanced_accuracy))/5\n",
    "    if cv_score > best_score: \n",
    "        best_weight = w\n",
    "        best_score = cv_score\n",
    "    res.append(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(np.linspace(1,30,50), res)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_weight)\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_balanced = LogisticRegression(solver='lbfgs', class_weight={0:1, 1:best_weight}) \n",
    "sum(cross_val_score(lr_balanced, X_train_scaled, y_train, cv=5, scoring=balanced_accuracy))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_balanced.fit(X_train_scaled, y_train)\n",
    "balanced_prediction = lr_balanced.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy for the 1-class\")\n",
    "print(1 - sum(abs(balanced_prediction[np.array(y_test) == 1] - 1))/sum(np.array(y_test) == 1))\n",
    "print()\n",
    "print(\"Accuracy for the 0-class\")\n",
    "print(1 - sum(abs(balanced_prediction[np.array(y_test) == 0] - 0))/sum(np.array(y_test) == 0))\n",
    "print()\n",
    "print(\"Balanced accuracy score: \")\n",
    "print(balanced_accuracy(lr_balanced, X_test_scaled, y_test))\n",
    "# Same as (acc_0 + acc_1)/2\n",
    "print()\n",
    "print(\"Weighted accuracy\")\n",
    "print(1 - sum(abs(balanced_prediction - y_test))/len(y_test))\n",
    "print()\n",
    "print(\"Recall\")\n",
    "print(recall(lr_balanced, X_test_scaled, y_test))\n",
    "print()\n",
    "print(\"Precision\")\n",
    "print(precision(lr_balanced, X_test_scaled, y_test))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_balanced.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_balanced.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(zip(X.columns, lr_balanced.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age Misclassification Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_ai = data[data[\"Origin\"] != \"AI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = age_misclassification(data[\"Age\"], data[\"clarifai_age\"]) == 1\n",
    "ms = age_misclassification(data[\"Age\"], data[\"microsoft_age\"]) == 1\n",
    "am = age_misclassification(data[\"Age\"], data[\"amazon_age_average\"]) == 1\n",
    "fa = age_misclassification(data[\"Age\"], data[\"face++_age\"]) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_misclass = (cl | ms | am |fa)\n",
    "sum(age_misclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[[\"Race\", \"Age\", \"Gender\", \"Emotion\"]]\n",
    "y = [int(x) for x in age_misclass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "X_eng = X.copy()\n",
    "\n",
    "is_black = [int(x) for x in X_eng[\"Race\"] == 2]\n",
    "is_white = [int(x) for x in X_eng[\"Race\"] == 4]\n",
    "is_asian = [int(x) for x in X_eng[\"Race\"] == 1]\n",
    "is_latino = [int(x) for x in X_eng[\"Race\"] == 3]\n",
    "is_rest = [int(x) for x in X_eng[\"Race\"] == 5]\n",
    "\n",
    "is_happy = [int(x) for x in X_eng[\"Emotion\"] == 5]\n",
    "is_sad = [int(x) for x in X_eng[\"Emotion\"] == 6]\n",
    "is_calm = [int(x) for x in X_eng[\"Emotion\"] == 2]\n",
    "is_angry = [int(x) for x in X_eng[\"Emotion\"] == 1]\n",
    "is_fearful = [int(x) for x in X_eng[\"Emotion\"] == 4]\n",
    "is_surprised = [int(x) for x in X_eng[\"Emotion\"] == 7]\n",
    "is_disgusted = [int(x) for x in X_eng[\"Emotion\"] == 3]\n",
    "\n",
    "X_eng[\"is_black\"] = is_black\n",
    "X_eng[\"is_white\"] = is_white\n",
    "X_eng[\"is_asian\"] = is_asian\n",
    "X_eng[\"is_latino\"] = is_latino\n",
    "X_eng[\"is_rest\"] = is_rest\n",
    "X_eng[\"is_happy\"] = is_happy\n",
    "X_eng[\"is_sad\"] = is_sad\n",
    "X_eng[\"is_calm\"] = is_calm\n",
    "X_eng[\"is_angry\"] = is_angry\n",
    "X_eng[\"is_fearful\"] = is_fearful\n",
    "X_eng[\"is_surprised\"] = is_surprised\n",
    "X_eng[\"is_disgusted\"] = is_disgusted\n",
    "\n",
    "X = X_eng.drop(columns=[\"Race\", \"Emotion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='lbfgs') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(cross_val_score(lr, X_train_scaled, y_train, cv=5))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train_scaled, y_train)\n",
    "prediction = lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_actually_misclass = np.array(y_test) == 1\n",
    "is_not_misclass = np.array(y_test) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(prediction[is_actually_misclass] == 1)/sum(is_actually_misclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(prediction[is_not_misclass] == 0)/sum(is_not_misclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_accuracy(estimator, X, y):\n",
    "    \"\"\" Calculate the precision of the estimator predicting on inputs X with output labels y\"\"\"\n",
    "    \n",
    "    prediction = np.array(estimator.predict(X))\n",
    "    y = np.array(y)\n",
    "    \n",
    "    is_actually_misclass = y == 1\n",
    "    is_not_misclass = y == 0\n",
    "    \n",
    "    acc_for_1 = sum(prediction[is_actually_misclass] == 1)/sum(is_actually_misclass)\n",
    "    acc_for_0 = sum(prediction[is_not_misclass] == 0)/sum(is_not_misclass)\n",
    "    \n",
    "    return (acc_for_0 + acc_for_1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='lbfgs') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(cross_val_score(lr, X_train_scaled, y_train, cv=5, scoring=balanced_accuracy))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "best_weight = None \n",
    "best_score = 0 \n",
    "for w in np.linspace(1,3,50): \n",
    "    lr_tune = LogisticRegression(solver='lbfgs', class_weight={0:1, 1:w})\n",
    "    cv_score = sum(cross_val_score(lr_tune, X_train_scaled, y_train, cv=5, scoring=balanced_accuracy))/5\n",
    "    if cv_score > best_score: \n",
    "        best_weight = w\n",
    "        best_score = cv_score\n",
    "    res.append(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(np.linspace(1,3,50), res)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_weight)\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_balanced = LogisticRegression(solver='lbfgs', class_weight={0:1, 1:best_weight}) \n",
    "sum(cross_val_score(lr_balanced, X_train_scaled, y_train, cv=5, scoring=balanced_accuracy))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_balanced.fit(X_train_scaled, y_train)\n",
    "balanced_prediction = lr_balanced.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy for the 1-class\")\n",
    "print(1 - sum(abs(balanced_prediction[np.array(y_test) == 1] - 1))/sum(np.array(y_test) == 1))\n",
    "print()\n",
    "print(\"Accuracy for the 0-class\")\n",
    "print(1 - sum(abs(balanced_prediction[np.array(y_test) == 0] - 0))/sum(np.array(y_test) == 0))\n",
    "print()\n",
    "print(\"Balanced accuracy score: \")\n",
    "print(balanced_accuracy(lr_balanced, X_test_scaled, y_test))\n",
    "# Same as (acc_0 + acc_1)/2\n",
    "print()\n",
    "print(\"Weighted accuracy\")\n",
    "print(1 - sum(abs(balanced_prediction - y_test))/len(y_test))\n",
    "print()\n",
    "print(\"Recall\")\n",
    "print(recall(lr_balanced, X_test_scaled, y_test))\n",
    "print()\n",
    "print(\"Precision\")\n",
    "print(precision(lr_balanced, X_test_scaled, y_test))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_balanced.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_balanced.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(zip(X.columns, lr_balanced.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Misclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_ai = data[data[\"Origin\"] != \"AI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = emotion_misclassification(data_no_ai[\"Emotion\"], data_no_ai[\"microsoft_emotion\"]) == 1\n",
    "am = emotion_misclassification(data_no_ai[\"Emotion\"], data_no_ai[\"amazon_emotion\"]) == 1\n",
    "fa = emotion_misclassification(data_no_ai[\"Emotion\"], data_no_ai[\"face++_emotion\"]) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_misclass = (ms | am | fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_no_ai[[\"Race\", \"Age\", \"Gender\", \"Emotion\"]]\n",
    "y = [int(x) for x in emotion_misclass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "X_eng = X.copy()\n",
    "\n",
    "is_black = [int(x) for x in X_eng[\"Race\"] == 2]\n",
    "is_white = [int(x) for x in X_eng[\"Race\"] == 4]\n",
    "is_asian = [int(x) for x in X_eng[\"Race\"] == 1]\n",
    "is_latino = [int(x) for x in X_eng[\"Race\"] == 3]\n",
    "is_rest = [int(x) for x in X_eng[\"Race\"] == 5]\n",
    "\n",
    "is_happy = [int(x) for x in X_eng[\"Emotion\"] == 5]\n",
    "is_sad = [int(x) for x in X_eng[\"Emotion\"] == 6]\n",
    "is_calm = [int(x) for x in X_eng[\"Emotion\"] == 2]\n",
    "is_angry = [int(x) for x in X_eng[\"Emotion\"] == 1]\n",
    "is_fearful = [int(x) for x in X_eng[\"Emotion\"] == 4]\n",
    "is_surprised = [int(x) for x in X_eng[\"Emotion\"] == 7]\n",
    "is_disgusted = [int(x) for x in X_eng[\"Emotion\"] == 3]\n",
    "\n",
    "X_eng[\"is_black\"] = is_black\n",
    "X_eng[\"is_white\"] = is_white\n",
    "X_eng[\"is_asian\"] = is_asian\n",
    "X_eng[\"is_latino\"] = is_latino\n",
    "X_eng[\"is_rest\"] = is_rest\n",
    "X_eng[\"is_happy\"] = is_happy\n",
    "X_eng[\"is_sad\"] = is_sad\n",
    "X_eng[\"is_calm\"] = is_calm\n",
    "X_eng[\"is_angry\"] = is_angry\n",
    "X_eng[\"is_fearful\"] = is_fearful\n",
    "X_eng[\"is_surprised\"] = is_surprised\n",
    "X_eng[\"is_disgusted\"] = is_disgusted\n",
    "\n",
    "X = X_eng.drop(columns=[\"Race\", \"Emotion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='lbfgs') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(cross_val_score(lr, X_train_scaled, y_train, cv=5))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train_scaled, y_train)\n",
    "prediction = lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_accuracy(estimator, X, y):\n",
    "    \"\"\" Calculate the precision of the estimator predicting on inputs X with output labels y\"\"\"\n",
    "    \n",
    "    prediction = np.array(estimator.predict(X))\n",
    "    y = np.array(y)\n",
    "    \n",
    "    is_actually_misclass = y == 1\n",
    "    is_not_misclass = y == 0\n",
    "    \n",
    "    acc_for_1 = sum(prediction[is_actually_misclass] == 1)/sum(is_actually_misclass)\n",
    "    acc_for_0 = sum(prediction[is_not_misclass] == 0)/sum(is_not_misclass)\n",
    "    \n",
    "    return (acc_for_0 + acc_for_1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='lbfgs') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(cross_val_score(lr, X_train_scaled, y_train, cv=5, scoring=balanced_accuracy))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "best_weight = None \n",
    "best_score = 0 \n",
    "for w in np.linspace(1,30,50): \n",
    "    lr_tune = LogisticRegression(solver='lbfgs', class_weight={0:1, 1:w})\n",
    "    cv_score = sum(cross_val_score(lr_tune, X_train_scaled, y_train, cv=5, scoring=balanced_accuracy))/5\n",
    "    if cv_score > best_score: \n",
    "        best_weight = w\n",
    "        best_score = cv_score\n",
    "    res.append(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(np.linspace(1,30,50), res)\n",
    "plt.xlabel(\"class_weight value\")\n",
    "plt.ylabel(\"Balanced accuracy\")\n",
    "#plt.savefig('training.svg', format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_weight)\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_balanced = LogisticRegression(solver='lbfgs', class_weight={0:1, 1:best_weight}) \n",
    "sum(cross_val_score(lr_balanced, X_train_scaled, y_train, cv=5, scoring=balanced_accuracy))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_balanced.fit(X_train_scaled, y_train)\n",
    "balanced_prediction = lr_balanced.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy for the 1-class\")\n",
    "print(1 - sum(abs(balanced_prediction[np.array(y_test) == 1] - 1))/sum(np.array(y_test) == 1))\n",
    "print()\n",
    "print(\"Accuracy for the 0-class\")\n",
    "print(1 - sum(abs(balanced_prediction[np.array(y_test) == 0] - 0))/sum(np.array(y_test) == 0))\n",
    "print()\n",
    "print(\"Balanced accuracy score: \")\n",
    "print(balanced_accuracy(lr_balanced, X_test_scaled, y_test))\n",
    "# Same as (acc_0 + acc_1)/2\n",
    "print()\n",
    "print(\"Weighted accuracy\")\n",
    "print(1 - sum(abs(balanced_prediction - y_test))/len(y_test))\n",
    "print()\n",
    "print(\"Recall\")\n",
    "print(recall(lr_balanced, X_test_scaled, y_test))\n",
    "print()\n",
    "print(\"Precision\")\n",
    "print(precision(lr_balanced, X_test_scaled, y_test))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
